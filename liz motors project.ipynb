{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60875a4",
   "metadata": {},
   "source": [
    "<span style=\"font-family: Arial; font-weight:bold;font-size:2em;color:#00b3e5;\"> Code for scrapping data\n",
    "    \n",
    "<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#00b3e5;\"> Submitted by\n",
    "    \n",
    "<span style=\"font-family: Arial; font-weight:bold;font-size:1.5em;color:#00b3e5;\"> Shameer.Sutar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "066d7e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canoo Inc. is an American automotive company based in Torrance, California, that develops and manufactures electric vehicles. Canoo's research & development team is based in Michigan, in the Detroit region (Auburn Hills, Livonia), and production operations are in Justin, Texas. The company also plans to produce commercial electric vehicles such as vans for fleet, vehicle rental and ride sharing services.\n",
      "\n",
      "\n",
      "== History ==\n",
      "Canoo was founded in 2017 under the name Evelozcity by Stefan Krause and Ulrich Kranz. Krause worked for Deutsche Bank as its chief financial officer while Kranz worked for BMW as a senior executive. Both men met at rival EV company Faraday Future before leaving together to form their own company in 2017, due to disagreement with Faraday Future's leadership. Krause took on the role of chief executive officer at Evelozcity, and Kranz became chief technology officer. The company received its primary funding from Chinese investor Li \"David\" Pak-Tam/Botan and German entrepreneur David Stern.In April 2018, Evelozcity Canoo hired Karl-Thomas Neumann, the former head of Opel as a senior executive. In March 2019 Evelozcity was renamed Canoo. In July 2019, Neumann left Canoo, but he remained an investor in the company.\n",
      "\n",
      "In September 2019, the company presented its first vehicle prototype, the electric van Canoo, which was later renamed to the Canoo Lifestyle Vehicle.In February 2020, Hyundai Motor Group, the parent company of Hyundai Motors and Kia Motors, announced that the company would partner with Canoo on the joint development of a new electric vehicle platform. The platform would be used for compact vehicles and for fleet vehicles such as shuttles. The deal is part of Hyundai's Strategy 2025 program which will see Hyundai investing US$87 billion for five years starting in 2020.In July 2020, co-founder Stefan Krause left the company. He had previously taken an extended leave of absence in August 2019 for family reasons. Co-founder Ulrich Kranz assumed the role of permanent CEO upon Krause's departure.In July 2020, Canoo featured on the TV program Jay Leno's Garage.In September 2020, Canoo announced a merger with the special-purpose acquisition company Hennessy Capital Acquisition Corp. IV., intending to list Canoo on the NASDAQ valued at $2.4 billion. The expectation was to raise $300 million to help finance the production of the Canoo minivan, planned for launch in 2022. On December 22, 2020, Canoo completed its merger with Hennessy Capital Acquisition Corp IV.A few days before its stock exchange debut, the company announced its MPDV (Multi-Purpose Delivery Vehicle) product line. The entry level van was expected to sell for $33,000 on arrival in limited quantities in 2022, ramping up to volume production in 2023.In mid-January 2021, The Verge reported that in the first half of 2020 Canoo had been in talks with Apple for a potential role in its secretive Titan car project.In May 2022, it was reported that Canoo was struggling to find funding, the company saying that it had only enough funding to operate for one more quarter. It was also revealed around the same time that Canoo was suing investor Pak Tim Li, claiming they were selling shares improperly.In December 2022, the company sued several former executives of stealing Canoo's trade secrets and poaching talent for their new business, competing EV startup Harbinger Motors.\n",
      "\n",
      "\n",
      "=== Corporate changes ===\n",
      "On March 11, 2021, Canoo announced the Canoo Pickup Truck, an electric pickup set to release in 2023. Canoo announced plans to offer both single-motor and dual-motor all-wheel drive options for their pickup truck, with the latter being capable of producing 600 hp (450 kW) and 550 foot-pounds (750 J) of torque. The company said that the truck would have over 200 miles (320 km) of range and a 1,800 lb (820 kg) payload capacity. After the announcement, shares of the company rose by 14%.It was announced in March 2021 that Canoo had terminated its partnership with Hyundai Motor Group due to a change in corporate strategy. The company also announced that it would shift away from vehicle subscriptions to selling commercial vehicles.On April 22, 2021, the company announced that co-founder and CEO Ulrich Kranz was stepping down and would be replaced by chairman Tony Aquila as CEO. Also in April 2021, the U.S. Securities and Exchange Commission launched an investigation into Canoo after its merger with Hennessy Capital Acquisition Corp. IV due to a string of executive departures, sudden changes to its business model, and class-action lawsuits brought by shareholders.On June 17, 2021, the company announced they would build a new factory in Pryor, Oklahoma (just outside Tulsa) to manufacture all of their future vehicles. The plant will be used to build \"pod-shaped vans it calls 'lifestyle vehicles' beginning in 2023.\" The same day, Dutch media reported that the Dutch company VDL Nedcar would start producing Canoo Minivans for the European market. Later in the year, Canoo and VDL ended the manufacturing agreement.On November 15, 2021, the company announced it would move its headquarters to Bentonville, Arkansas and establish a manufacturing plant there. As of December 2022 the company still lists Torrance, California as its headquarters.In January 2022, Canoo entered into a 10-year US$17.7 million lease for a building in Bentonville, Arkansas for an \"advanced industrialization facility\" to be used for low-volume manufacturing. In August 2022, Canoo disclosed they had contracted with a third party for their initial vehicle production.\n",
      "Following receipt of orders in October 2022, Canoo announced plans for construction of a vehicle battery production facility at the MidAmerica Industrial Park (MAIP) in Pryor, Oklahoma. This was in addition to its earlier announcement of plans for a vehicle production plant at MAIP capable of producing 300,000 vehicles per year. In late November 2022, Canoo announced an agreement to purchase of an existing 630,000 square foot plant in Oklahoma City to start vehicle production by 2023, prior to completion of its micro megafactory in Pryor.  In April 2023, the company announced that battery production would proceed at Pryor to fulfill a United States Department of Defense contract; and, while vehicle manufacturing was to start in Oklahoma City before the end of 2023, long range plans still included vehicle assembly at Pryor because the OKC facility would not meet full production needs. The Oklahoma City plant transaction was completed on April 7, 2023; the plant was formerly owned and operated by Terex.\n",
      "\n",
      "\n",
      "=== Firm orders ===\n",
      "On April 13, 2022, NASA selected Canoo to supply crew transportation vehicles for its Artemis program, with a total contract value of $147,855.In July 2022, Walmart entered into a definitive agreement to purchase 4,500 all-electric Lifestyle Delivery Vehicles (LDV) from Canoo, with an option to buy up to 10,000 in the future. Under the terms, Walmart can terminate the agreement for convenience upon 30 days notice.Canoo announced two major orders in mid-October 2022. Zeeba, a relatively unknown fleet leasing operation, also based in Greater Los Angeles, ordered a combination of 5,450 LDVs and Lifestyle Vehicles (LV), with a binding commitment of 3,000 by 2024. Some days later, Canoo announced its biggest sale to date, an order from Kingbee, a Utah-based work-ready van fleet rental company. In addition to the binding order of 9,300 LDVs, Kingbee has the option to double the order. Kingbee outfits its vans for fleet customers in construction trades (electrical, plumbing, energy), delivery, and mobile health.In January 2024, Canoo announced the Postal Service (USPS) order for six right-hand-drive LDV vans.\n",
      "\n",
      "\n",
      "== References ==\n",
      "\n",
      "\n",
      "== External links ==\n",
      "Official website Business data for Canoo Inc.:\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import csv\n",
    "\n",
    "# Search Wikipedia for Canoo the electric vehicle company  \n",
    "result = wikipedia.page(\"Canoo (electric vehicle manufacturer)\") \n",
    "\n",
    "# Extract full content of the Wikipedia page\n",
    "full_text = result.content\n",
    "\n",
    "print(full_text)\n",
    "\n",
    "# Write the scraped data to a CSV file\n",
    "with open('canoo_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Overview\"]) \n",
    "    writer.writerow([full_text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e548418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully scraped and saved to 'canoo_data.txt'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import random\n",
    "\n",
    "# Function to search queries and retrieve URLs using Google search\n",
    "def search_queries(query, num_results=5):\n",
    "    urls = []\n",
    "    try:\n",
    "        counter = 0\n",
    "        for url in search(query, stop=num_results):\n",
    "            urls.append(url)\n",
    "            counter += 1\n",
    "            if counter >= num_results:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching search results for '{query}': {e}\")\n",
    "    return urls\n",
    "\n",
    "# Function to scrape data from URLs using Selenium WebDriver\n",
    "def scrape_data(urls):\n",
    "    scraped_data = []\n",
    "    options = Options()\n",
    "    options.headless = True  # Run Firefox in headless mode (without opening a browser window)\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    for url in urls:\n",
    "        retries = 3  # Number of retries\n",
    "        delay = 1  # Initial delay in seconds\n",
    "        timeout = 70  # Default timeout\n",
    "        if \"hayekcollege.com\" in url:\n",
    "            timeout = 70  # Increase timeout for problematic URL\n",
    "        elif \"statista.com\" in url:\n",
    "            timeout = 70  # increase timeout for problematic URL\n",
    "        driver.set_page_load_timeout(timeout)  # Set timeout dynamically based on URL\n",
    "\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                # Check if the soup object is None\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to parse HTML content for {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Extracting title from webpage\n",
    "                title = soup.title.text.strip() if soup.title else \"Title Not Found\"\n",
    "\n",
    "                # Extracting content\n",
    "                content = None\n",
    "                if soup.body:\n",
    "                    content = soup.body.get_text(separator='\\n').strip()\n",
    "\n",
    "                # If no content found, log an error and continue\n",
    "                if not content:\n",
    "                    print(f\"No content found for {url}\")\n",
    "                    continue\n",
    "\n",
    "                scraped_data.append({'Title': title, 'Content': content})\n",
    "                break  # Break the retry loop if successful\n",
    "            except TimeoutException:\n",
    "                print(f\"Page load timed out while scraping {url}. Retrying...\")\n",
    "            except NoSuchElementException as e:\n",
    "                print(f\"No content found for {url}: {e}\")\n",
    "                break  # Break the retry loop if NoSuchElementException occurs\n",
    "            except WebDriverException as e:\n",
    "                print(f\"WebDriver error occurred while scraping {url}: {e}\")\n",
    "                break  # Break the retry loop if WebDriverException occurs\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while scraping {url}: {e}\")\n",
    "                break  # Break the retry loop if unexpected error occurs\n",
    "            finally:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    # Exponential backoff\n",
    "                    delay *= 2\n",
    "                    delay += random.uniform(0, 1)  # Add random jitter\n",
    "                    time.sleep(delay)\n",
    "\n",
    "    driver.quit()  # Quit the WebDriver session after scraping all URLs\n",
    "    return scraped_data\n",
    "\n",
    "# Function to save data into a text file\n",
    "def save_to_text(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as output_file:\n",
    "        for item in data:\n",
    "            output_file.write(f\"Title: {item['Title']}\\n\")\n",
    "            output_file.write(f\"Content: {item['Content']}\\n\\n\")\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    # Queries to search\n",
    "    queries = [\n",
    "        \"Canoo industry overview\",\n",
    "        \"Canoo competitors analysis\",\n",
    "        \"Electric vehicle market trends\",\n",
    "        \"Canoo financial performance\"\n",
    "    ]\n",
    "\n",
    "    # Perform searches and scrape data\n",
    "    all_data = []\n",
    "    for query in queries:\n",
    "        urls = search_queries(query)\n",
    "        data = scrape_data(urls)\n",
    "        all_data.extend(data)\n",
    "\n",
    "    # Save scraped data into a text file\n",
    "    if all_data:\n",
    "        save_to_text(all_data, 'canoo_data.txt')\n",
    "        print(\"Data successfully scraped and saved to 'canoo_data.txt'\")\n",
    "    else:\n",
    "        print(\"No data scraped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad4163b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No content found for https://www.wsj.com/market-data/quotes/GOEV/financials\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import PyPDF2\n",
    "from googlesearch import search\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import random\n",
    "\n",
    "# Function to search queries and retrieve URLs using Google search\n",
    "\n",
    "def search_queries(query, num_results=5):\n",
    "    urls = []\n",
    "    try:\n",
    "        counter = 0\n",
    "        for url in search(query, stop=num_results):\n",
    "            urls.append(url)\n",
    "            counter += 1\n",
    "            if counter >= num_results:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching search results for '{query}': {e}\")\n",
    "    return urls\n",
    "\n",
    "# Function to extract text from PDF URLs\n",
    "def extract_text_from_pdf(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        with open(filename, 'rb') as f:\n",
    "            reader = PyPDF2.PdfFileReader(f)\n",
    "            text = ''\n",
    "            for page_num in range(reader.numPages):\n",
    "                text += reader.getPage(page_num).extractText()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(title):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', title)\n",
    "\n",
    "# Function to scrape data from URLs using Selenium WebDriver\n",
    "\n",
    "def scrape_data(urls):\n",
    "    scraped_data = []\n",
    "    options = Options()\n",
    "    options.headless = True  # Run Firefox in headless mode (without opening a browser window)\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    for url in urls:\n",
    "        retries = 3  # Number of retries\n",
    "        delay = 1  # Initial delay in seconds\n",
    "        timeout = 70  # Default timeout\n",
    "        if \"pitchbook.com\" in url:\n",
    "            timeout = 120  # Increase timeout for problematic URL\n",
    "        elif \"macroaxis.com\" in url:\n",
    "            timeout = 120  # Increase timeout for problematic URL\n",
    "        driver.set_page_load_timeout(timeout)  # Set timeout dynamically based on URL\n",
    "\n",
    "        while retries > 0:\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                # Check if the soup object is None\n",
    "                if soup is None:\n",
    "                    print(f\"Failed to parse HTML content for {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Extracting title from webpage\n",
    "                title = soup.title.text.strip() if soup.title else \"Title Not Found\"\n",
    "\n",
    "                # Extracting content\n",
    "                content = None\n",
    "                if soup.body:\n",
    "                    content = soup.body.get_text(separator='\\n').strip()\n",
    "\n",
    "                # If no content found, log an error and continue\n",
    "                if not content:\n",
    "                    print(f\"No content found for {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Sanitize title for use as filename\n",
    "                filename = f\"{sanitize_filename(title)}.txt\"\n",
    "\n",
    "                # Save content to a file\n",
    "                with open(filename, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "                scraped_data.append({'Title': title, 'Content': content, 'File': filename})\n",
    "                break  # Break the retry loop if successful\n",
    "            except TimeoutException:\n",
    "                print(f\"Page load timed out while scraping {url}. Retrying...\")\n",
    "            except NoSuchElementException as e:\n",
    "                print(f\"No content found for {url}: {e}\")\n",
    "                break  # Break the retry loop if NoSuchElementException occurs\n",
    "            except WebDriverException as e:\n",
    "                print(f\"WebDriver error occurred while scraping {url}: {e}\")\n",
    "                break  # Break the retry loop if WebDriverException occurs\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while scraping {url}: {e}\")\n",
    "                break  # Break the retry loop if unexpected error occurs\n",
    "            finally:\n",
    "                retries -= 1\n",
    "                if retries > 0:\n",
    "                    # Exponential backoff\n",
    "                    delay *= 2\n",
    "                    delay += random.uniform(0, 1)  # Add random jitter\n",
    "                    time.sleep(delay)\n",
    "\n",
    "    driver.quit()  # Quit the WebDriver session after scraping all URLs\n",
    "    return scraped_data\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "\n",
    "def main():\n",
    "    # Queries to search\n",
    "    queries = [\n",
    "        \"Canoo industry overview\",\n",
    "        \"Canoo competitors analysis\",\n",
    "        \"Electric vehicle market trends\",\n",
    "        \"Canoo financial performance\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Perform searches and scrape data\n",
    "    all_data = []\n",
    "    for query in queries:\n",
    "        urls = search_queries(query)\n",
    "        data = scrape_data(urls)\n",
    "        all_data.extend(data)\n",
    "\n",
    "    # Save scraped data into a text file\n",
    "    with open('canoo_data1.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for item in all_data:\n",
    "            output_file.write(f\"Title: {item['Title']}\\n\")\n",
    "            output_file.write(f\"Content: {item['Content']}\\n\")\n",
    "            output_file.write(f\"File: {item['File']}\\n\\n\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076267de",
   "metadata": {},
   "source": [
    "**scrapping data into csv's was difficult because of fowllowing reasons hence stored in text files**\n",
    "\n",
    ">> **Structured Data Handling:** CSV files are more suitable for structured data where each row represents a record and each column represents a field. However, scraped data from websites often come in unstructured or semi-structured formats, such as HTML or plain text. Parsing and converting such unstructured data into a structured format suitable for CSVs can be complex and error-prone.\n",
    "\n",
    ">> **Data Integrity:** CSV files assume a tabular structure where each row has the same number of fields. In web scraping, however, the data obtained from different web pages may vary significantly in structure and content. Ensuring data integrity and consistency across different scrapes and handling variations in data format can be challenging when storing in CSV format.\n",
    "\n",
    ">> **Special Characters and Delimiters:** CSV files use commas or other delimiters to separate fields. If the scraped data contains special characters or the same delimiter used in the content, it can lead to misinterpretation of data or parsing errors. Proper escaping or encoding of special characters is necessary to avoid such issues, adding complexity to the data storage process.\n",
    "\n",
    ">> **Data Loss:** CSV format may not preserve all the information obtained during web scraping, especially if the data contains nested structures, rich text, or multimedia content. Storing such complex data in CSV format may result in loss of information or data truncation, leading to loss of valuable insights.\n",
    "\n",
    ">> **Additional Processing:** After scraping data from websites, further processing such as data cleaning, normalization, or transformation may be required before storing in CSV format. This additional processing adds complexity and overhead to the data storage pipeline.\n",
    "\n",
    "Due to these challenges, storing scraped data in text files (e.g., plain text or HTML format) is often preferred in web scraping tasks. Text files offer more flexibility in handling unstructured data, preserving the original content without loss of information, and requiring less preprocessing compared to CSV files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc58452",
   "metadata": {},
   "source": [
    "**Web scraping, or the process of extracting data from websites, can be challenging due to several reasons: and faced such problems**\n",
    "\n",
    ">> **Website Structure Variability:** Websites are built using various technologies (HTML, CSS, JavaScript, etc.) and may have different structures and layouts. This variability makes it challenging to create a single scraping script that works effectively across all websites. Developers often need to customize scraping scripts for each website or even for different pages within the same website.\n",
    "\n",
    ">> **Dynamic Content Loading:** Many modern websites use dynamic content loading techniques, such as AJAX or JavaScript frameworks like React or Angular. These techniques asynchronously fetch data from the server and update the webpage without reloading the entire page. Traditional web scraping tools may struggle to interact with and extract data from dynamically loaded content, requiring more advanced techniques such as headless browsers or browser automation.\n",
    "\n",
    ">> **Anti-Scraping Measures:** To prevent automated access and scraping, websites may implement various anti-scraping measures such as CAPTCHA challenges, rate limiting, IP blocking, or user-agent detection. Dealing with these measures requires sophisticated scraping techniques and may involve rotating IP addresses, using proxy servers, or employing CAPTCHA-solving services.\n",
    "\n",
    ">> **Session Management and Authentication:** Some websites require users to log in or maintain session cookies to access certain pages or data. Scraping authenticated content or handling session management adds complexity to the scraping process, as it involves handling cookies, managing user sessions, and dealing with login forms.\n",
    "\n",
    ">> **Legal and Ethical Concerns:** Web scraping may raise legal and ethical concerns, as scraping copyrighted or sensitive data without permission may violate website terms of service or copyright laws. Scraping large amounts of data from a website may also put strain on the website's servers and impact its performance, leading to potential legal issues or ethical considerations.\n",
    "\n",
    ">> **Data Quality and Consistency:** The quality and consistency of scraped data can vary widely depending on factors such as website design, data formatting, and changes in website layout or content. Scraped data may contain errors, missing information, or inconsistencies, requiring data cleaning and preprocessing before analysis or use.\n",
    "\n",
    ">> **Maintenance Overhead:** Websites are frequently updated, redesigned, or restructured, which can break existing scraping scripts. Maintaining and updating scraping scripts to adapt to changes in website structure or content requires ongoing effort and monitoring.\n",
    "\n",
    "Despite these challenges, web scraping remains a powerful tool for extracting valuable data from the web for various purposes such as market research, competitive analysis, and data-driven decision-making. Overcoming these challenges often involves a combination of technical expertise, adaptability, and adherence to legal and ethical standards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be945e",
   "metadata": {},
   "source": [
    "### Apart from these chanlleges i was able to scrape some amount of data using above coding scripts about canno inc company and made a report  based on the scrapped data and completed the task as yu requested sir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe109729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
